{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyojinko/2022_CV_Project/blob/main/Colorization_ResUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4wuV0fI1sq5",
        "outputId": "3e286b92-773b-4e61-e47d-b30dafc1e7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Jr3FqR810ZY"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import tqdm\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kz8JXpAc19Zd"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/MyDrive/ComputerVision/colorization_dataset.zip'\n",
        "file_name = 'colorization_dataset.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "\n",
        "!unzip -q '{file_name}'\n",
        "!rm '{file_name}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "roGXvpfs1-Bq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ColorHintTransform(object):\n",
        "  def __init__(self, size=256, mode=\"train\"):\n",
        "    super(ColorHintTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "    self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "  def bgr_to_lab(self, img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, ab = lab[:, :, 0], lab[:, :, 1:]\n",
        "    return l, ab\n",
        "\n",
        "  def hint_mask(self, bgr, threshold=[0.95, 0.97, 0.99]):\n",
        "    h, w, c = bgr.shape\n",
        "    mask_threshold = random.choice(threshold) # 3 threshold random choice\n",
        "    mask = np.random.random([h, w, 1]) > mask_threshold # Create a mask with only hint values\n",
        "    return mask\n",
        "\n",
        "  def img_to_mask(self, mask_img):\n",
        "    mask = mask_img[:, :, 0, np.newaxis] >= 255\n",
        "    return mask\n",
        "\n",
        "  def __call__(self, img, mask_img=None):\n",
        "    threshold = [0.95, 0.97, 0.99]\n",
        "    if (self.mode == \"train\") | (self.mode == \"val\"):\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      mask = self.hint_mask(image, threshold)\n",
        "\n",
        "      hint_image = image * mask # hint_image we know\n",
        "\n",
        "      l, ab = self.bgr_to_lab(image) # split image into l and ab\n",
        "      l_hint, ab_hint = self.bgr_to_lab(hint_image) # split hint_image into l and ab\n",
        "      return self.transform(l), self.transform(ab), self.transform(ab_hint), self.transform(mask) # l, ab, ab_hint, mask transform apply # Add mask\n",
        "\n",
        "\n",
        "    elif self.mode == \"test\":\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      mask = self.img_to_mask(mask_img)\n",
        "      hint_image = image * self.img_to_mask(mask_img)\n",
        "      l, _ = self.bgr_to_lab(image)\n",
        "      _, ab_hint = self.bgr_to_lab(hint_image)\n",
        "      \n",
        "      return self.transform(l), self.transform(ab_hint), self.transform(mask)\n",
        "\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uPR01k3d2AoT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class ColorHintDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(ColorHintDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "    self.hint = None\n",
        "    self.mask = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = ColorHintTransform(self.size, mode)\n",
        "    if mode == \"train\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"val\":\n",
        "      val_dir = os.path.join(self.root_path, \"val\")\n",
        "      self.examples = [os.path.join(self.root_path, \"val\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif self.mode == \"test\":\n",
        "            hint_dir = os.path.join(self.root_path, \"hint\")\n",
        "            mask_dir = os.path.join(self.root_path, \"mask\")\n",
        "            self.hint = [os.path.join(self.root_path, \"hint\", dirs) for dirs in os.listdir(hint_dir)]\n",
        "            self.mask = [os.path.join(self.root_path, \"mask\", dirs) for dirs in os.listdir(mask_dir)]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "  def __len__(self):\n",
        "    if self.mode != \"test\":\n",
        "      return len(self.examples)\n",
        "    else:\n",
        "      return len(self.hint)\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode == \"test\":\n",
        "      hint_file_name = self.hint[idx]\n",
        "      mask_file_name = self.mask[idx]\n",
        "      hint_img = cv2.imread(hint_file_name)\n",
        "      mask_img = cv2.imread(mask_file_name)\n",
        "\n",
        "      input_l, input_hint, input_mask = self.transforms(hint_img, mask_img)\n",
        "      sample = {\"l\": input_l, \"hint\": input_hint,\"mask\": input_mask,\n",
        "                      \"file_name\": \"image_%06d.png\" % int(os.path.basename(hint_file_name).split('.')[0])}\n",
        "    else:\n",
        "      file_name = self.examples[idx]\n",
        "      img = cv2.imread(file_name)\n",
        "      l, ab, hint, mask = self.transforms(img) # Add mask\n",
        "      sample = {\"l\": l, \"ab\": ab, \"hint\": hint, \"mask\": mask} # Add mask\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spqIzMQ42CG0",
        "outputId": "765c68fe-fcbc-46af-bae6-fc961e0f4dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train length :  10000\n",
            "Validation length :  2000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8): # Tensor type -> image type\n",
        "  if isinstance(input_image, torch.Tensor):\n",
        "      image_tensor = input_image.data\n",
        "  else:\n",
        "      return input_image\n",
        "  image_numpy = image_tensor[0].cpu().float().numpy()\n",
        "  if image_numpy.shape[0] == 1:\n",
        "      image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "  image_numpy = np.clip((np.transpose(image_numpy, (1, 2, 0)) ),0, 1) * 255.0\n",
        "  return image_numpy.astype(imtype)\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/cv_project\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "# Get dataset\n",
        "train_dataset = ColorHintDataset(root_path, 256)\n",
        "train_dataset.set_mode(\"train\")\n",
        "print('Train length : ', len(train_dataset))\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "val_dataset = ColorHintDataset(root_path, 256)\n",
        "val_dataset.set_mode(\"val\")\n",
        "print('Validation length : ', len(val_dataset))\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define ResUNet**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8d9WVeWjVNZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class upsample(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(upsample, self).__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Conv2d(ch_in,ch_out,3,1,1),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True ),\n",
        "            nn.ConvTranspose2d(ch_out , ch_out , 3,2,1,1),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "252BgBE6Vgas"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.RCNN = nn.Sequential(\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out)\n",
        "        )\n",
        "        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Conv_1x1(x)\n",
        "\n",
        "        x1 = self.RCNN(x)\n",
        "        return x + x1"
      ],
      "metadata": {
        "id": "d0_unjaaVpRF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, upsample, downsample, ch_result):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.Conv2d(upsample, ch_result, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(ch_result)\n",
        "        )\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Conv2d(downsample, ch_result, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(ch_result)\n",
        "        )\n",
        "\n",
        "        self.concat = nn.Sequential(\n",
        "            nn.Conv2d(ch_result, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        skip1 = self.skip(g)\n",
        "\n",
        "        up1 = self.up(x)\n",
        "        attention = self.relu(skip1 + up1)\n",
        "        attention = self.concat(attention)\n",
        "\n",
        "        return x * attention"
      ],
      "metadata": {
        "id": "AGvRtBuTV9Bi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, img_ch=4, output_ch=3):\n",
        "        super(ResUNet, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Dsample1 = ResidualBlock(ch_in=img_ch, ch_out=64 )\n",
        "        self.Dsample1_1 = ResidualBlock(ch_in=64 , ch_out=64)\n",
        "\n",
        "        self.Dsample2 = ResidualBlock(ch_in=64, ch_out=128)\n",
        "        self.Dsample2_1 = ResidualBlock(ch_in=128, ch_out=128)\n",
        "\n",
        "        self.Dsample3 = ResidualBlock(ch_in=128, ch_out=256)\n",
        "        self.Dsample3_1 = ResidualBlock(ch_in=256, ch_out=256)\n",
        "\n",
        "        self.Dsample4 = ResidualBlock(ch_in=256, ch_out=512 )\n",
        "        self.Dsample4_1 = ResidualBlock(ch_in=512, ch_out=512)\n",
        "\n",
        "        self.Dsample5 = ResidualBlock(ch_in=512, ch_out=1024)\n",
        "        self.Dsample5_1 = ResidualBlock(ch_in=1024, ch_out=1024)\n",
        "\n",
        "        self.Up5 = upsample(ch_in=1024, ch_out=512)\n",
        "        self.Attention5 = AttentionBlock(upsample=512, downsample=512, ch_result=256)\n",
        "        self.Usample5 = ResidualBlock(ch_in=1024, ch_out=512)\n",
        "        self.Usample5_1 = ResidualBlock(ch_in=512, ch_out=512)\n",
        "\n",
        "        self.Up4 = upsample(ch_in=512, ch_out=256)\n",
        "        self.Attention4 = AttentionBlock(upsample=256, downsample=256, ch_result=128)\n",
        "        self.Usample4 = ResidualBlock(ch_in=512, ch_out=256 )\n",
        "        self.Usample4_1 = ResidualBlock(ch_in=256, ch_out=256)\n",
        "\n",
        "        self.Up3 = upsample(ch_in=256, ch_out=128)\n",
        "        self.Attention3 = AttentionBlock(upsample=128, downsample=128, ch_result=64)\n",
        "        self.Usample3 = ResidualBlock(ch_in=256, ch_out=128 )\n",
        "        self.Usample3_1 = ResidualBlock(ch_in=128, ch_out=128)\n",
        "\n",
        "        self.Up2 = upsample(ch_in=128, ch_out=64)\n",
        "        self.Attention2 = AttentionBlock(upsample=64, downsample=64, ch_result=32)\n",
        "        self.Usample2 = ResidualBlock(ch_in=128, ch_out=64)\n",
        "        self.Usample2_1 = ResidualBlock(ch_in=64, ch_out=64)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # encoding path\n",
        "        x1 = self.Dsample1(x)\n",
        "        x1 = self.Dsample1_1(x1)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Dsample2(x2)\n",
        "        x2 = self.Dsample2_1(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Dsample3(x3)\n",
        "        x3 = self.Dsample3_1(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Dsample4(x4)\n",
        "        x4 = self.Dsample4_1(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.Dsample5(x5)\n",
        "        x5 = self.Dsample5_1(x5)\n",
        "\n",
        "        d5 = self.Up5(x5)\n",
        "        x4 = self.Attention5(g=d5, x=x4)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Usample5(d5)\n",
        "        d5 = self.Usample5_1(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        x3 = self.Attention4(g=d4, x=x3)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Usample4(d4)\n",
        "        d4 = self.Usample4_1(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        x2 = self.Attention3(g=d3, x=x2)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Usample3(d3)\n",
        "        d3 = self.Usample3_1(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        x1 = self.Attention2(g=d2, x=x1)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Usample2(d2)\n",
        "        d2 = self.Usample2_1(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "id": "fYI0vIHQ3EQ4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o1HjQzbu2rvk"
      },
      "outputs": [],
      "source": [
        "def train_1epoch(net, train_dataloader):\n",
        "  epoch = 0\n",
        "  total_loss = 0\n",
        "  for i, data in enumerate(tqdm.auto.tqdm(train_dataloader)):\n",
        "    l = data['l'].to('cuda')\n",
        "    ab = data['ab'].to('cuda')\n",
        "    hint  = data['hint'].to('cuda')\n",
        "    mask = data[\"mask\"].cuda()\n",
        "\n",
        "    gt_img = torch.cat((l, ab), dim=1).cuda()\n",
        "    hint_img = torch.cat((l, hint, mask), dim=1).cuda()\n",
        "    output_hint = net(hint_img)\n",
        "    loss = criterion(output_hint, gt_img)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.detach()\n",
        "    epoch += 1\n",
        "\n",
        " \n",
        "\n",
        "  total_loss /= epoch\n",
        "  total_loss = total_loss.cpu()\n",
        "  total_loss = total_loss.numpy()\n",
        "  return  total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W43EkqJM24iM"
      },
      "outputs": [],
      "source": [
        "def val_1epoch(net, val_dataloader):\n",
        "  total_loss = 0\n",
        "  epoch = 0\n",
        "  net.eval()\n",
        "  i = 0\n",
        "  for i, data in enumerate(tqdm.auto.tqdm(val_dataloader)):\n",
        "    l = data['l'].to('cuda')\n",
        "    ab = data['ab'].to('cuda')\n",
        "    hint = data['hint'].to('cuda')\n",
        "    mask = data['mask'].to('cuda')\n",
        "\n",
        "    gt_img = torch.cat((l, ab), dim=1).cuda()\n",
        "\n",
        "    hint_img = torch.cat((l, hint, mask), dim=1).cuda()\n",
        "    output_hint = net(hint_img)\n",
        "    loss = criterion(output_hint, gt_img)\n",
        "    total_loss += loss.detach()\n",
        "\n",
        "\n",
        "    gt_np = tensor2im(gt_img)\n",
        "    hint_np = tensor2im(output_hint)\n",
        "  \n",
        "    \n",
        "\n",
        "    gt_bgr = cv2.cvtColor(gt_np, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    hint_bgr = cv2.cvtColor(hint_np, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    os.makedirs('/content/cv_project/label',exist_ok=True)\n",
        "    cv2.imwrite('/content/cv_project/label/gt_'+str(epoch+1)+'.jpg',gt_bgr)\n",
        "\n",
        "    os.makedirs('/content/cv_project/predictions',exist_ok=True)\n",
        "    cv2.imwrite('/content/cv_project/predictions/pred_'+str(epoch+1)+'.jpg',hint_bgr)\n",
        "\n",
        "    epoch += 1\n",
        "  total_loss /= epoch\n",
        "  total_loss = total_loss.cpu()\n",
        "  total_loss = total_loss.numpy()\n",
        "  cv2_imshow(gt_bgr)\n",
        "  cv2_imshow(hint_bgr)\n",
        "  return  total_loss\n",
        "\n",
        "best_losses = 10\n",
        "net = ResUNet().cuda()\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './Result'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "output_path = os.path.join(save_path, 'validation_model.tar')\n"
      ],
      "metadata": {
        "id": "ZIhNyWeo2mTU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training ResUNet**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H9wfhup--EHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import mse_loss as mse   \n",
        "\n",
        "net = ResUNet().cuda()\n",
        "lr = 0.0001\n",
        "object_epoch = 10\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "def psnr(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:\n",
        "  if not isinstance(input, torch.Tensor):\n",
        "      raise TypeError(f\"Expected torch.Tensor but got {type(target)}.\")\n",
        "\n",
        "  if not isinstance(target, torch.Tensor):\n",
        "      raise TypeError(f\"Expected torch.Tensor but got {type(input)}.\")\n",
        "\n",
        "  if input.shape != target.shape:\n",
        "      raise TypeError(f\"Expected tensors of equal shapes, but got {input.shape} and {target.shape}\")\n",
        "\n",
        "  return 10. * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))\n",
        "\n",
        "def psnr_loss(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:\n",
        "  return -1. * psnr(input, target, max_val)\n",
        "\n",
        "# ================== class PSNRLoss ================== \n",
        "\n",
        "class PSNRLoss(nn.Module):\n",
        "  def __init__(self, max_val: float) -> None:\n",
        "    super(PSNRLoss, self).__init__()\n",
        "    self.max_val: float = max_val\n",
        "\n",
        "  def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    return psnr_loss(input, target, self.max_val)\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "criterion = PSNRLoss(2.)\n",
        "# criterion = nn.BCELoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.00025) # 1e-2 # 0.0005 # 0.00025 # 0.0002\n",
        "epochs = 150 \n",
        "best_losses = 10\n",
        "\n",
        "\n",
        "net.cuda()\n",
        "\n",
        "val_losses = 100\n",
        "train_info = []\n",
        "val_info = []\n",
        "\n",
        "for epoch in range(object_epoch):\n",
        "  train_loss = train_1epoch(net, train_dataloader)\n",
        "  print('Epoch: {} loss: {}'.format(epoch, train_loss))\n",
        "  train_info.append({'loss':train_loss})\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_loss = val_1epoch(net, val_dataloader)\n",
        "  print('[VALIDATION] Epoch: {} loss: {}'.format(epoch, val_loss))\n",
        "  val_info.append({'loss':val_loss})\n",
        "\n",
        "  if best_losses > val_loss:\n",
        "    best_losses = val_loss\n",
        "    torch.save(net.state_dict(), os.path.join(save_path,'PSNR-epoch-{}-losses-{:.5f}.pth'.format(epoch + 1, best_losses)))\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "979705c94cdd4685b8737ffc10014aa6",
            "33a1198e00ab4e7f8d1f025a2bf2acda",
            "8eacbf04244d4bf083688224f1c3d904",
            "d8b48ef3570942d7a62906b15e55169c",
            "53f5365f36bd426983476e1b128e00a9",
            "df6aaf9e6997464bbd03b80a6fa448fe",
            "3f53a75a87e942339420b1a426be9580",
            "38baf59dc79f43119569cf51f5f05def",
            "1c58aeea5dad4fdf9955ef23f1ae98bb",
            "53563d6f3eb343fabcb4f7133d9f3e3b",
            "f42f877ed30f4621af8d5958d2902d66"
          ]
        },
        "id": "X249342L-Hdf",
        "outputId": "d00a1ebc-18fb-45c5-83ce-117dc68ada8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "979705c94cdd4685b8737ffc10014aa6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PZBixufZ90wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'test_dataset.zip'\n",
        "zip_path = os.path.join('/content/drive/MyDrive/ComputerVision/test_dataset.zip')\n",
        "\n",
        "!cp '{zip_path}' .\n",
        "!unzip -q '{file_name}'"
      ],
      "metadata": {
        "id": "661Ute8893s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "def image_save(img, path):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    img = np.asarray(transforms.ToPILImage()(img))\n",
        "  \n",
        "  img = cv2.cvtColor(img, cv2.COLOR_LAB2BGR)\n",
        "  cv2.imwrite(path, img)\n",
        "\n",
        "check_point = ''\n",
        "use_cuda = True\n",
        "\n",
        "test_dataset = ColorHintDataset(root_path, 256)\n",
        "test_dataset.set_mode('test')\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "net = ResUNet().cuda()\n",
        "net.load_state_dict(torch.load(check_point))\n",
        "os.makedirs('outputs/predict', exist_ok=True)\n",
        "os.makedirs('outputs/predict/test', exist_ok=True)\n",
        "def test(model, test_dataloader):\n",
        "  model.eval()\n",
        "  for i, data in enumerate(test_dataloader):\n",
        "    l = data['l'].cuda()\n",
        "    hint = data['hint'].to('cuda')\n",
        "    mask = data['mask'].to('cuda')\n",
        "    file_name = data['file_name']\n",
        "    with torch.no_grad():\n",
        "      hint_img = torch.cat((l, hint, mask), dim=1)\n",
        "      output_hint = net(hint_img)\n",
        "      out_hint_np = tensor2im(output_hint)\n",
        "      output_bgr = cv2.cvtColor(out_hint_np, cv2.COLOR_LAB2BGR)\n",
        "      fname = str(file_name).replace(\"[\",'')\n",
        "      fname = fname.replace(\"']\", '')\n",
        "      cv2.imwrite('outputs/predict/test/'+str(fname),output_bgr)\n",
        "\n",
        "test(net, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XdtYWYCDXFy",
        "outputId": "1367b71d-574e-4e6c-affc-546bdfd1b1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test length : 1000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Colorization_ResUNet_0524.ipynb의 사본의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsk7kDXMkmdIssBkrFQd7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "979705c94cdd4685b8737ffc10014aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33a1198e00ab4e7f8d1f025a2bf2acda",
              "IPY_MODEL_8eacbf04244d4bf083688224f1c3d904",
              "IPY_MODEL_d8b48ef3570942d7a62906b15e55169c"
            ],
            "layout": "IPY_MODEL_53f5365f36bd426983476e1b128e00a9"
          }
        },
        "33a1198e00ab4e7f8d1f025a2bf2acda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6aaf9e6997464bbd03b80a6fa448fe",
            "placeholder": "​",
            "style": "IPY_MODEL_3f53a75a87e942339420b1a426be9580",
            "value": " 54%"
          }
        },
        "8eacbf04244d4bf083688224f1c3d904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38baf59dc79f43119569cf51f5f05def",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c58aeea5dad4fdf9955ef23f1ae98bb",
            "value": 1355
          }
        },
        "d8b48ef3570942d7a62906b15e55169c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53563d6f3eb343fabcb4f7133d9f3e3b",
            "placeholder": "​",
            "style": "IPY_MODEL_f42f877ed30f4621af8d5958d2902d66",
            "value": " 1354/2500 [07:50&lt;06:38,  2.88it/s]"
          }
        },
        "53f5365f36bd426983476e1b128e00a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6aaf9e6997464bbd03b80a6fa448fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f53a75a87e942339420b1a426be9580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38baf59dc79f43119569cf51f5f05def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c58aeea5dad4fdf9955ef23f1ae98bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53563d6f3eb343fabcb4f7133d9f3e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f42f877ed30f4621af8d5958d2902d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}